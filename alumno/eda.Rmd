---
title: "Exploratory Data Analysis: EDA"
output: html_notebook
---




```{r, message=FALSE, warning=FALSE, include=FALSE}
rm(list = ls())

instalar <- function(paquete) {

    if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
        install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
        library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
}

paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest', 
              'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
              'GGally')

lapply(paquetes, instalar);

source("metadata.R")
source("utils.R")
```

# GDA

## Tipos de variables

- Numéricas continuas

- No numéricas
    - Categóricas nominales
    - Categóricas ordinales
    - Datos discretos
        - Conteos y Enteros

- *Texto*, *Imágenes*, *Audio*, etc.
    - Ver *feature engineering* más adelante en el curso


## Exploratory Data Analysis

- Es importante conocer tus datos **siempre**
- Es principalmente usada para ayudarte con el modelado
- Pero es clave para la limpieza de los datos, tranformación, etc.
- También será  importante para la automatización de los modelos 
- Servirá para detectar si exite algún *drift* de los datos.
- Una parte muy importante es hacer esto **visual** (por lo menos las primeras veces): **GEDA**.
    - Las secciones siguientes (hasta antes de **Casos de estudio**) están basadas en el libro *Graphical Data Analysis with R*.

### Algunos consejos

- No existe un tipo óptimo de gráfica, prueba varias.
- Siempre debes de pensar que esperas ver, antes de dibujar la gráfica


### Variables contínuas

- ¿Qué podemos encontrar?
    - Asimetría
    - *Outliers*
    - Multimodalidad
    - Huecos (*Gaps*)
    - *Heaping*
    - Redondeos
    - Imposibilidades
    - Errores

- ¿Cómo visualizarlas?
    - *Histogramas* Muestran la distribución empírica (*raw*)
        - ¡*Empírica*!  No son estimadores...
    - *Boxplot* Despliega *outliers*. Estadística Robusta (tener cuidado con el paquete que se utilice). Comparación entre subgrupos.
    - *Dotplot* Cada observación un punto, permnite detectar *huecos* en los datos
    - *Rugplot* Cada observación como una pequeña línea en el eje horizontal, se usa en conjunto con otras gráficas
    - *Density estimate* Es como un *modelo* de los datos
         - Sólo recuerda los límites de las variables, sobre si todo si son estrictos: *siempre* revisa la documentación.
    - *Distribution estimate* Útil para comparar distribuciones (e.g. ver si una está adelante de la otra)
    - *QQ plot* Compara la distribución contra una distribución teórica (por *default* es la distribución normal)


### Ejemplos    

El *dataset* `Boston` del paquete **MASS** contiene 14 variables de 506 áreas alrededor de la ciudad de Boston.

```{r}
glimpse(MASS::Boston)
```

Una de sus principales variables es el valor medio de las casas por área, la varoable `medv`. Podemos 
utilizar visualizarlo con una tabla, pero salvo el hecho de que todo está redondeado a un decimal, no 
es fácil sacar más información.

```{r}
with(MASS::Boston, table(medv))
```


    
```{r}
with(MASS::Boston, hist(medv))
```
    

Hay dos cosas que notar en esta gráfica, la caída alreddor de 25 y lo inusual que 
está el valor en 50. ¿Quizá sea por el tamaño del *bin*?
    
```{r}
ggplot(MASS::Boston, aes(medv)) + 
    geom_histogram() +
    ylab("") + xlab("Valor medio de las casas (1000s USD)")
```

```{r}
ggplot(MASS::Boston, aes(medv)) + 
    geom_histogram(binwidth=1) +
    ylab("") + xlab("Valor medio de las casas (1000s USD)")
```

¿Qué pasa con las demás variables?

```{r}
B2 <- gather(MASS::Boston, BosVars, BosValues, crim:medv)
ggplot(B2, aes(BosValues)) +
    geom_histogram() + xlab("") + ylab("") +
    facet_wrap(~ BosVars , scales = "free")
```
Esta gráfica sirve como punto inicial, pero el tamaño de los *bines* no beneficia a todas las variables 
por igual, además la escala varía muchísimo entre las variables (entre 40 y 400). Por último
el `anchorpoint` (i.e. donde empieza el primer *bin* y si está abierto por la izquiera o la derecha)
puede ser importante, como muestra el siguiente ejemplo:

```{r}
with(MASS::Boston, hist(ptratio, freq = FALSE))
```

```{r}
with(MASS::Boston, MASS::truehist(ptratio))
```

## Ejercicio

1. Grafica `medv` usando `boxplot`, `stripchart` (*jittered dotplot*), `stem`, `density` estimate con  `rug` plot

```{r}
?boxplot
?stripchart
?stem
?density
?rug
```

¿Qué observas en cada una de estas gráficas?

2. En la gráfica de las 14 variables mostrada arriba ¿Cómo describirías las distribuciones? 
  ¿Para cuales variables sería mejor utilizar `boxplot`? ¿Por qué?


Los *outliers* son difíciles de tratar de manera automática y son muy importantes de resolver

```{r}
glimpse(ggplot2movies::movies)
```


```{r}
ggplot(ggplot2movies::movies, aes(length)) + geom_histogram() + ylab("")
```

El histograma no resultó de mucha utilidad en esta ocasión.

```{r}
ggplot2movies::movies %>% 
    filter(length > 2000) %>%
    select(length, title) %>%
    arrange(desc(length))
```

```{r}
ggplot(ggplot2movies::movies, aes("var", length)) + 
    geom_boxplot() + xlab("") + scale_x_discrete((breaks = NULL)) + coord_flip()
```

Podemos ver el histograma si recortamos esas variables

```{r}
ggplot(ggplot2movies::movies, aes(x=length)) + 
    xlim(0, 180) +
    geom_histogram(binwidth = 1) +
    xlab("Duración de películas en minutos") + ylab("") 
```
*Ejercicio* 

1. ¿Qué puedes decir de esta gráfica?
2. ¿Cómo la modificas para agregar más *ticks*?
3. Haz una gráfica que muestre que los picos a los 7 y 90 minutos existían antes y después de 1980
4. Existe la variable `Short` que indica si la película es "corta" ¿Qué gráfica puedes  hacer para
   ver que criterio se utilizó para definir esta variable y cuáles están mal clasificadas?
    
## Variables categóricas

- ¿Qué podemos encontrar?
    - Patrones insesperados en los resultados
    - Malas Distribuciones 
    - Categorías extras
    - Experimentos no balanceados
    - Muchas categorías
    - "No sé", Errores, Faltantes
    
- ¿Cómo visualizarlas?
    - *Barchars*: Nominales, Ordinales, Variables discretas
    - Explora (o ten cuidado con) diferentes ordenamientos

## Dos variables contínuas

- ¿Qué podemos encontrar?
    - Relaciones lineales y no lineales
    - Asociaciones
    - *Outliers* 
        - Se puede ser *outlier* en una dimensión y no serlo en dos, o viceversa
    - *Clusters*
    - *Gaps*
    - Barreras
    - Relación condicional

- Modelos y pruebas
    - Correlación
    - Regresión lineal
    - *Smoothing*

## Ejemplos

Podemos continuar con la base de datos `movies`.  En particular veamos las variables `rating`,
la cual representa el promedio de calificaciones en **IMDB** y `votes`, número de personas que
calificaron la película.

```{r}
ggplot(ggplot2movies::movies, aes(votes, rating)) + geom_point() + ylim(1,10)
```

*Ejercicio*
1. Agrega *alpha-blending* ¿Qué pasa con los  *outliers*? ¿Diferentes valores funcionan mejor?
1. ¿Cómo se ve la gráfica si  remueves las películas con menos de 100 votos?
2. ¿Cómo si remueves todas las películas que tienen un *rating* arriba de 9?

Al igual que en el caso univariado, es posible estudiar posibles modelos, como se muestra en el siguiente ejemplo,
usando el *dataset* `Cars93` del paquete **MASS**:

```{r}
ggplot(MASS::Cars93, aes(Weight, MPG.city)) + geom_point() +
    geom_smooth(colour="green") + ylim(0,50)
```

*Ejercicio*
- ¿Cuál es el *outlier* de la izquierda?
- En muchos países en lugar de medirse el desempeño en millas por galón, se mide  en 
  litros por 100 km. ¿Qué pasa si graficas `1/MPG.city` contra `Horsepower`? ¿Existe una relación 
  lineal?¿Cuáles son los outliers?
  
Al igual que cuando graficamos todas las variables del *dataset* `Boston`, podemos hacer un
**splom** (*Scatterplot matrix*)

```{r, fig.height=13, fig.width=13}
MASS::Boston %>%
    select(-rad,-chas) %>%
    ggpairs(title="Boston Dataset", diag=list(continuos='density', axisLabels='none'))
```

*Ejercicio*
- ¿Cuáles están positivamente correlacionadas con `medv`?
- La variable `crim` (tasa de crímenes per cápita) tiene *scatterplots* con forma inusual,
  donde los valores más altos de `crim` sólo ocurren para una valor de la otra variable
  ¿Qué explicación le puedes dar?
- Hay varias formas en los *scatterplots*, escoge 5 y explica como las interpretas?

## Más de dos variables contínuas

Básicamente lo que queda aquí es el *parallel coordinate plot* (**pcp**). Estos 
diagramas dan vistazos rápidos a las distribuciones univariadas de varias variables a la vez:
si son *skew*, si hay *outliers*, si hay *gaps*, etc.

El famoso `iris` *dataset* nos permite ver algunas cosas con este tipo de gráfica

```{r}
ggparcoord(iris, columns = 1:4, groupColumn = "Species")
```

Hay varias cosas a notar, primero, lo que una observa en la gráfica depende del orden 
en el cual se dibujan los ejes. Hay autores que sugieren intentar varias combinaciones o 
inclusive poner varias copias de los ejes. Quizá lo mejor sea tener una gráfica 
interactiva para tal efecto (ver más adelante en el curso).

Otro tema es el de *outliers* y escalamiento de las variables. Este tipo de gráficas
se puede usar para comparar modelos, series de tiempo, analizar *clusters*, índices, etc
Estos temas los discutiremos más adelante también.


## Varias variables categóricas

...

## Forma *deseable*  de los datos

Para mucho de los análisis queremos que los datos estén en formato *tidy*.

- Ver el artículo [**Tidy Data**](http://vita.had.co.nz/papers/tidy-data.pdf) de Hadley Wickham

- Pero hay ejemplos donde no queremos *tidy data* e.g. [**Non Tidy Data**](http://simplystatistics.org/2016/02/17/non-tidy-data/)

- O grafos (ver más adelante)
    


![Tidy Data](http://r4ds.had.co.nz/images/tidy-1.png)

*Fuente: R for Data Science, Wickham and Grolemund, 2016*

Es decir:

1. Cada *variable* una columna
2. Cada *observación* un renglón
3. Cada *valor* una celda

## Proceso: ¿Dónde estamos?

![Limpieza y Transformación](../images/Data_Analysis_and_Cleaning.png)

*Fuente: Presentaciones de Hadley Wickham, por ejemplo [esta](http://vita.had.co.nz/papers/tidy-data-pres.pdf)*

## Formatos *on the wild*

![Indeseable pero Real 1](../images/formatos_indeseables_1.png)

![Indeseable pero Real 2](../images/formatos_indeseables_2.png)

![Indeseable pero Real 3](../images/formatos_indeseables_3.png)


Otros ejemplos:

- Nombres de las columnas representan valores de los datos en lugar de nombres de variables
- Una columna contiene varias variables en lugar de una variable
- Una tabla contiene más de una unidad de observación
- Las variables están contenidas en los renglones y columnas, en lugar de sólo columnas.
- Los datos de una unidad observacional están dispersas en varios *data sets*





# Casos de estudio

## ¿Quién eres? 

Eres el científico de datos de un banco alemán, el banco tiene muchas pérdidas debido a malos créditos y quiere reducirlas. 
Te piden realizar esta tarea, indicando que quieren reducir la tasa de pérdidas en un 10%.

## Datos  

Usaremos para este ejemplo, los datos de crédito alemán (*German data set*). 
[German Credit Data](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)


## Carga de datos {.tiny}

```{r, message=FALSE, warning=FALSE}

german_url <- paste('http://archive.ics.uci.edu/ml',
                    '/machine-learning-databases/statlog',
                    '/german/german.data',
                    sep='')

german_data <- read_delim(german_url, 
                          col_names=FALSE, 
                          delim = " ")
```

Los datos son un asco...

```{r}
german_data
```

## Ejercicio 

- Crea una función `load` en `utils.R` en tu carpeta, que descargue, si y sólo si no existe
un archivo `german.rds`. Si no existe, descarga y guarda el archivo.

- `?saveRDS`, `?readRDS`

## Transformación de datos
Los nombres de las columnas fueron copiados a mano desde [`german.doc`](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc)

```{r}
german_colnames
```

## Transformación de datos

La variable de salida la estoy definiendo como categórica (`factor` en `R`)
```{r}
colnames(german_data) <- german_colnames

german_data$good_loan <- as.factor(
                          ifelse(
                            german_data$good_loan == 1, 
                            'GoodLoan', 
                            'BadLoan'
                            )
                          )
```

## Decodificar

- Crea una función `german_decode` en un archivo `utils.R` dentro de tu carpeta, 
esta función debe de utilizar `german_codes` (en el archivo `metadata.R`) para 
decodificar los elementos  de todas las columnas (por ejemplo `A201` -> `yes`)

- Utiliza `dplyr` para decodificar todas las columnas de `german_data`

```{r, warning=FALSE}
german_data  <- german_data %>% 
                     mutate_each(funs(german_decode))

german_data
```


## Datos manejables 

En este momento deberás de tener archivos `0-load.R`, `1-prepare.R`, `metadata.R` 
y un archivo `utils.R` dentro de `german`.  
Además deberías de tener un archivo `german.rds`.

## Ejercicio 

- ¿Hay algo raro con los datos de préstamo?

- ¿Cuál crees que debería ser la distribución del resultado del 
  préstamo `Good_Loan` respecto a `Credit history`?

- Grafícalo y comenta tus resultados.

- Si lo vas a hacer con `ggplot2` usa este 
[cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf) 
o estos [ejemplos](http://docs.ggplot2.org/current/)

## Ejercicio

- Fue terrible poder hacer la gráfica con `ggplot2` utilizando los nombres de
columnas que pusimos (`german_colnames`).

- Modifica el archivo donde tengas `german_colnames` (puede ser `utils.R` o `metadata.R`) y 
sustituye (usando quizá `stringr` o `grep`) los `' '` y `'/'` por `'_'` (ve la [guía de 
estilo](http://adv-r.had.co.nz/Style.html)) y pasa todo a minúsculas.

- Ejecuta todo de nuevo (¡la ventaja de ser reproducible!)

```{r}
colnames(german_data) <- german_clean_colnames(german_colnames)
```


## Intermedio

- Si te quedó desacomodado, este código ordena los `bar charts`

```{r}
german_data %>% 
    group_by(credit_history) %>% 
    dplyr::summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    ggplot(.) + 
        geom_bar(aes(x=reorder(credit_history, count), y = count), stat="identity", fill="gray") + 
        coord_flip() + 
        theme_hc() + 
        ylab('casos') + 
        xlab('Historial de crédito')
```

## Sanidad de los datos

- El nombre de la columna no significa lo que tu crees que significa

- El significado de la columna cambia con el paso del tiempo o la metodología para medir esa variable.

- Mucha / muy poca resolución

- Los valores `missing` no son realmente faltantes (`NAs`), si no que significan algo
    - Regularmente no documentado
    
- Si es un `csv` de seguro a alguien ya le pareció chistoso ponerle comas 
    - Existe una historia parecida para los `tsv`, `psv`, etc.


## `summary`

Un uso del  `summary()` es detectar problemas en los datos.

- ¿Valores faltantes?
    - ¿Hay una variable con muchos faltantes? ¿Por qué? ¿Es un error? ¿Significa algo?
    
- ¿Valores inválidos?
    - ¿Hay negativos donde no debería de haber? (Como en edad, ingreso, estatura)
    - ¿Texto en lugar de números?


- ¿Outliers?
    - Son aquellos valores que no crees que deberían de estar (En el ejemplo de edad 1400 años)


## `summary`

- ¿Rangos?
    - Es importante saber cuanto varía la variable.
    - Si es muy amplio, puede ser un problema para algunos algoritmos de modelado.
    - Si varía muy poco (o nada) no puede ser usado como predictor.
    
- ¿Unidades?
    - ¿El salario es mensual?¿Quincenal?¿Por hora?
    - ¿Los intervalos de tiempo están en segundos?¿Años?
    - ¿Las longitudes? ¿La moneda?


## Ejercicio 

Revisa `german_data` con `summary()`, reporta alguna anomalía.


```{r}
summary(german_data)
```


## ¿Quién eres? 

Como el dinero no alcanza, tomas otro trabajo rápido para una ONG. Quieren predecir 
la concentración de algas en ríos de la región. Tomaron datos durante un año. 

Cada observación es el efecto de agregar varias muestras de agua recolectadas 
en el mismo río por un periodo de 3 meses en la misma estación del año.


## Datos 

Los datos provienen de [Coil 1999 Competition 
Data](https://archive.ics.uci.edu/ml/datasets/Coil+1999+Competition+Data) sobre contaminación de ríos.
La explicación de los datos se puede ver [aquí](https://archive.ics.uci.edu/ml/machine-learning-databases/coil-mld/coil.data.html)

```{r}
algas_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/coil-mld/analysis.data'

algas <- read_csv(algas_url, 
                  col_names = algas_colnames,
                  na = 'XXXXXXX')
```


## Ejercicio 

- Repite los pasos realizados para `german.data` con `algas`

- No te olvides de remover los `_` en las variables `river_size` y `fluid_velocity`

- Revisa con `summary()`, reporta alguna anomalía.



```{r}
summary(algas)
```

¿Por qué la columna `NO3` **no** es numérica?

```{r}
problems(algas)
```

El problema lo podemos observar, por ejemplo, en la columna `20`

```{r}
algas[20,]
```

Otra cosa interesante a notar, es que hay justo `r length(problems(algas))` casos de `a7` con `NA`s. 
Al parecer el error en la columna de `NO3` se está "comiendo" a la columna `a7`.

```{r}
algas[problems(algas)$row,]
```

La columna de `NO3` está capturada a 5 decimales. Parece lógica la suposición de 
dividir esa columna a los 5 decimales.

Podemos limpiar esta columna haciendo lo siguiente

```{r}
problematic_rows <- problems(algas)$row

algas[problematic_rows,] <- algas %>% 
    slice(problematic_rows) %>% 
    unite(col="all", -seq(1:6), sep = "/", remove=TRUE) %>%
    extract(all, into=c("NO3", "NH4", "resto"), regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=TRUE) %>%
    separate(resto, into=names(algas)[9:18], sep="/", remove=TRUE)    

#algas[problems(algas)$row,] %>% unite(col="all", -seq(1:6), sep = "/", remove=TRUE) %>%
#extract(all, into=c("NO3", "NH4", "resto"), regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=TRUE) %>%
#separate(c, into=names(algas)[9:18], sep="/", remove=TRUE)
#
algas
```


```{r}
#algas$NO3 <- str_match(string=algas$NO3, pattern="[0-9]*.[0-9]*")

algas <- readr::type_convert(algas)

algas
```


```{r, warning=FALSE}
algas <- algas %>%
            mutate_each(funs(algas_clean))

algas
```


Revisando el resumen estadístico con  `summary`:

```{r}
summary(algas)
```

Los tipos de datos:

```{r}
glimpse(algas)
```


## Usando gráficas

- El resumen estadístico quizá no cuente toda la historia.

- El siguiente paso es explorar mediante gráficas.

- Es un proceso iterativo.

## Una sola variable 

- ¿Cuál es el pico? ¿Coincide con la media? ¿La mediana? ¿Existe?
- ¿Cuántos picos?
  - Si es `bimodal` o `multimodal` quizá haya varias poblaciones en lugar de una y será mejor modelar por separado.
- ¿Qué tan normal es? ¿El log-normal? 
    - Utiliza una gráfica `Q-Q`
- ¿Cuánto varía? ¿Está concentrada en un intervalo o una categoría?
- ¿Outliers? -> Usa gráficas de `boxplot`.
- Da preferencia a los `density plots`, en esta gráfica es más importante la forma que los valores actuales del eje vertical.
- Si los datos están concentrados en un solo lado de la gráfica (`skewed`) y es no negativa es bueno representarla en `log10`.
- Una grafica de barras no da más información que  `summary()`, aunque alguna gente las prefiere.
    - Es bueno mostrarla horizontal y ordenada.

## Ejercicio

- Es importante en la etapa de exploración, poder generar varias gráficas de manera automática y simple para analizarlas visualmente y tener una idea de los datos. 
- Crea una función que genere los tipos de gráfica correspondiente para cada variable del `data.frame`. 
- Guárdala en `utils.r`. 
- Úsala en `2-eda.r` en ambas carpetas: `algas` y `german`.

## Dos variables
- ¿Existe relación entre dos variables de entrada? ¿entre una entrada y la variable de salida?
- ¿Qué tan fuerte?
- ¿Qué tipo de relación?
- `Scatter plot` entre dos variables numéricas, calcular la correlación de Pearson en un conjunto `sano` de los datos, visualizar la curva que mejor representa los datos.
- `Stacked bar charts` para dos variables categóricas.
    - Si quieres comparar razones a lo largo de las categorías lo mejor es usar un `filled bar chart`. En este caso se recomienda agregar un  `rug` para tener una idea de la cantidad de individuos.
    - Si hay múltiples categorías por variable, es mejor usar `facets`.
- Para variable categórica y numérica es recomendable usar `boxplot` (en su versión de `violin` o `jitter`).

## Ejercicio

- Es importante en la etapa de exploración, poder generar varias gráficas de manera automática y simple para analizarlas visualmente y tener una idea de los datos. 
- Crea una función que genere los tipos de gráfica para cada par de variables del `data.frame`. Esta función debe de recibir dos parámetros, uno que indique si genera todas las combinaciones de dos variables o recibe una lista de variables en las cuales generar las combinaciones.
- Guárdala en `utils.R`. 
- Úsala en `2-eda.R` en ambas carpetas: `algas` y `german`.


## Valores faltantes: `NAs`

- Los pasos son los siguientes:
    - Identificar los datos faltantes.
    - Examinar las causas de los datos faltantes. 
        - Preguntar al domain expert, etc.
    - Borrar los casos (o columnas) que contienen los `NAs` o reemplazar (imputar) los `NAs` con valores razonables.
    
- La teoría la veremos más adelante en el curso.

<span class="yellow">NOTA: Todas estás recomendaciones aplican igual para outliers</span>

## Valores faltantes: `NAs`

- Es importante recordar que en `R` la operación `x == NA` nunca regresa `TRUE`, siempre hay que utilizar las funciones `is.na()`, `is.nan()` e `is.infinite()`.

- El método `complete.cases` identifica los renglones (individuos) del `data.frame` que no tienen ningún `NA` en sus columnas (variables).

- Es posible usar `sum` y `mean` con `is.na` para obtener el total por columna de faltantes y el porcentaje.
    - ¿Por qué?
    
## Valores faltantes

Aunque más adelante veremos técnicas más poderosas, vale la pena mencionar 
el ejemplo mostrado en `R in action`, cap. 15. 

La técnica nos permite determinar si los faltantes en una variable están correlacionados con otra.
```{r, eval=FALSE}
x <- as.data.frame(abs(is.na(df))) # df es un data.frame

head(df)

head(x)

# Extrae las variables que tienen algunas celdas con NAs
y <- x[which(sapply(x, sd) > 0)] 

# Da la correación un valor alto positivo significa que desaparecen juntas.
cor(y) 
```

## Ejercicio 

- Genera un reporte para ambos conjuntos de datos el estado de los valores missing.

- Muestra la matriz de correlación faltante en una gráfica.

- ¿Qué puedes entender?

## Variables faltantes: Remover observaciones {.tiny}

Las variables con más faltantes son Promedio de Cloruro `Cl` (10) y Promedio de Clorofila `Chla` (12).

```{r}
summary(algas[-grep(colnames(algas),pattern = "^a[1-9]")]) # Nota el uso del grep
```

También se puede hacer con `dplyr`

```{r}
algas %>%
    select(-starts_with("a")) %>%
    summary()
```



## Variables faltantes: Remover observaciones 

Antes de removerlas es recomendable verlos, guardarlos y contarlos:

```{r, eval=FALSE}
nrow(algas[!complete.cases(algas),])
```
Hay `r nrow(algas[!complete.cases(algas),])` observaciones en las cuales tienen `NAs`

```{r}
algas.con.NAs <- algas[!complete.cases(algas),]
```
Siempre es bueno guardarlas, si se piensan eliminar del dataset.

## Variables faltantes: Remover observaciones {.tiny}

Las observaciones con `NAs`son las siguientes (usaremos la función `print()` para explorar)

```{r}
algas.con.NAs[c('max_PH', 'min_O2', 'Cl', 'NO3', 'NH4', 'oPO4', 'PO4', 'Chla')]  %>%
    print(n = 33)
```

de los cuales, hay dos renglones (`16` y `33`)que tienen más del `50%` (`6`) de las variables independientes nulas.

Aunque remover las observaciones con `NAs` **NO** sea la estrategia, quitar las observaciones con muchas columnas vacías, puede ser recomendable.

## Variables faltantes: Remover observaciones
En los casos en los que no es posible hacer una explorarción visual, se puede utilizar el siguiente código
```{r}
# ¿Cuántos NAs hay por observación?
apply(algas, 1, function(x) sum(is.na(x)))
```
Si queremos ver las observaciones:
```{r}
algas[apply(algas, 1, function(x) sum(is.na(x))) > 2,]
```
Lo cual confirma nuestra exploración visual.

## Variables faltantes: Remover observaciones 

Si eliminar las observaciones con `NAs` va a ser el camino que vamos a tomar, habrá que hacerlo de manera
reproducible, lo que sigue es el código de la función `indices_con_NAs'

```{r}
indices_con_NAs
```


## Variables faltantes: Remover observaciones 

```{r}
indices_con_NAs(algas, 0.2)
```

```{r}
indices_con_NAs(algas, 0.8)
```

```{r, eval=FALSE}
# Si queremos remover las que tengan más del 20% de NAs...
algas <- algas[-indicesConNAs(algas, 0.2),]
```

## Variables faltantes: Renivelar 

- Si la variable es categórica (`factor`), puedes crear una nueva variable y poner los `NA`s a un nuevo `level`, e.g. `missing`

    - Por ejemplo, suponiendo que hubiese una variable categórica con faltantes en nuestros dataset
  
```{r eval=FALSE}
dataset$cat_with_NAs_fix <- ifelse(is.na(dataset$cat_with_NAs),
                              "missing",
                              ifelse(dataset$ccat_with_NAs == TRUE,    
                                                # o el valor que sea
                                                "level_1",
                                                "level_2"))
```   

- Sólo recuerda que es posible que el valor de `NA` signifique algo.

- Esto también se puede hacer con variables numéricas, si primero las vuelves categóricas (i.e. *binning*)

## Variables faltantes: Central

- Una estrategia es rellenar los valores faltantes con alguna medida de centralidad.
    - Media, mediana, moda, etc.

- Para variables distribuidas normalmente, esta opción es la mejor.

- Pero para variables *skewed*  o con *outliers* esta decisión puede ser desastrosa.

- Por lo tanto, esta estrategia no se debe de utilizar salvo una exploración previa de las variables.

## Ejemplo

- ¿A qué variables le puedes de `algas` le puedes aplicar este procedimiento?

- ¿Qué puedes decir de `german_data`?

- A las variables que no se les puede aplicar, explica por qué no.

- Esta decisión debe de ser reproducible, agrega a `utils.R` una función que impute 
en las variables con  `NAs` el valor central (`median` si es numérica, `moda` si es categórica).
La función debe de tener la siguiente firma:

```{r, eval=FALSE}
imputar_valor_central <- function(data, colnames) {...}
```

## Variables faltantes: Correlación

Calculando rápidamente la correlación

```{r}
algas %>%
    select(-c(1:3)) %>%
    cor(use="complete.obs") %>%
    symnum()
```

Observamos que `NO3` y `NH4` y `oPO4` y `PO4` están altamente relacionadas (`> 0.9`).

Si removiste las observaciones no hay columnas que tengan vacíos a `NO3` y `NH4` a la vez.


## Variables faltantes: Correlación
```{r correlacion, warning=FALSE, fig.height=4, fig.width=8}
ggplot(data=algas) + 
  aes(x=oPO4, y=PO4) + 
  geom_point(shape=1) + # Usamos una bolita para los puntos
  geom_smooth(method=lm, se=FALSE) +
    theme_hc()
  # Mostramos la linea de la regresión y no mostramos la región de confianza
```

## Variables faltantes: Correlación

```{r}
algas <- algas[-indices_con_NAs(algas, 0.2),]
modelo <- lm(PO4 ~ oPO4, data=algas)
modelo
```

Entonces la fórmula que relaciona el `PO4` con `oPO4` es

$$
PO4 = `r modelo$coefficients['(Intercept)']` + `r modelo$coefficients['oPO4']`*oPO4
$$

**Ejercicio**: Crea una función que sustituya los `NAs` con el valor dado por la 
regresión lineal recién calculada (No automatices la regresión lineal) usando la
siguiente firma

```{r, eval=FALSE}
imputar_valor_lm <- function(var_independiente, modelo) { ... }
```


## Variables faltantes: Similitud

- Podemos suponer que si dos observaciones son similares y una de ellas tiene `NAs` 
en alguna variable, hay una alta probabilidad de que esa variable tenga un valor
similar al valor de esa variable en la otra observación.
    - Obviamente es una suposición...
    
- Debemos definir la noción de similar
    - Y esto significa definir un espacio métrico en el espacio que
       usamos para describir las observaciones.
    - Obviamente, otra gran suposición...

## Variables faltantes: Similitud

- Para variables numéricas se puede usar la distancia euclídea

$$
d(\vec{x}, \vec{y}) = \sqrt{\sum_{i=1}^p(\vec{x}_i - \vec{y}_i)}
$$

- Si son nominales las variables

$$
d(\vec{x}, \vec{y}) = \sqrt{\sum_{i=1}^p \delta_i(\vec{x}_i, \vec{y}_i)}
$$

donde $\delta(\vec{x},  \vec{y})$ es a delta de Kronecker.

## Variables faltantes: Similitud

- Una vez definida la similitud, debemos de definir el valor que imputar al `NA`.
    
- Una opción es utilizar una medida de centralidad de los $k$ observaciones más cercanas. 

- El Promedio con peso de los valores de los vecinos, es otra opción. El peso se puede determinar de varias
maneras, pero usar como *kernel* una función gaussiana.

$$
peso(d) = e^{-d}
$$

donde $d$ es la distancia de una observación a la que estamos considerando.

## Variables faltantes: Similitud

- Es importante normalizar los valores numéricos antes de calcular las distancias.

$$
\vec{x}_{normalizado} = \frac{\vec{x}_i - \bar{x}}{\sigma_{x}}
$$

**Ejercicio:** ¿Por qué?


## Tarea 

- Formen equipos de dos personas, implementen una función que impute por similitud con la firma

```{r, eval=FALSE}
imputar_por_similitud <- function(data, num_vecinos) { ... }
```

- Aplíquelo a `algas` y `german`. 

- ¿Son muy diferentes las estadísticas ignorando los `NAs` comparadas con este método?

- Agréguelo a su reporte de exploración de ambos datasets. 
  Explíque cuál método de imputación usaste y porqué.

## Resumen de EDA  

- En general la exploración de datos se divide en tres pasos:
    - Verificar la distribución de las variables individuales
        - Identificando *outliers*, valores faltantes $\to$ transformación, eliminación del *dataset*, etc.
    - Verificar la relación entre las variables dependientes y los predictores 
        - Se podrá usar en `feature selection`
    - Relación entre los predictores
        - Eliminación de variables redundantes

# Preparación para modelado

## Hasta ahora... 

- Deberías de contar en este momento con dos documentos, de análisis exploratorio. 
  Estos documentos son internos y son para que te familiarices con los datos. 

- El siguiente paso es preparar los datos. Esta actividad también genera un reporte, 
  pero este reporte servirá (además de bitácora) como herramienta de comunicación 
  con otros equipos y de estandarización para el modelado.
    - e.g. Podría ser la base de un documento de diseño de ETL.

- En la carpeta `plantillas` está el archivo `data_preparation.Rmd` 
  este funge como plantilla para el reporte. 

## Tarea

- ¿Recuerdas el ejercicio de arreglar los nombres de variables? Crea una función 
  que lo haga, y además agrega la funcionalidad de que cambie 
  los nombres de variables `camelCase` a `snake_case`.

- La firma de dicha función debe de ser

```{r, eval=FALSE}
normalizarNombres <- function(nombres_de_columnas) { ... }
```

**NOTA**: El paquete `rattle` tiene una función llamada `nomVarNames` 
que hace exactamente esto, puedes ver su código para inspirarte.

- En este momento es quizá una buena idea, dejar de duplicar código y concentrar t
  odas las funciones de `utils.R` que se puedan reutilizar en un archivo `functions.R`.

## Transformación de datos 

- Normalizar
    - Es útil cuando las cantidades absolutas son menos importantes que las relativas.
    
- Normalizar y reescalar 
    - Usar la desviación estándar como unidad de medida.
    - Tiene mucho sentido si la distribución es simétrica.
    - Si no lo es, es posible que sea *lognormally distributed* 
     (como el ingreso monetario o los gastos), una transformación `log10()`
     lo hará útil.

## Transformación de datos 

- Es una buena idea usar `log` si el rango de tus datos cubre varios ordenes de magnitud. 
    - Regularmente, estas variables vienen de procesos **multiplicativos** en lugar de aditivos. 

- Si el rango incluye cantidades negativas, usa (crea) una función `signedLog10`

```{r eval=FALSE}
signedLog10 <- function(x) {
  ifelse(abs(x) <= 1.0, sign(x)*log10(abs(x)))
}
```

## Columnas de procedencia  

- Un aspecto que siempre es olvidado, o que no se considera importante debido a los `blogs`,
  es el **versionado de control de los datos**

- Esto se puede implementar, agregando columnas para indicar de donde vienen los datos, 
  o con qué procedimiento de limpieza se generaron, etc.
    - Se puede utilizar el mismo `id` del código del ETL guardado en `github`.
